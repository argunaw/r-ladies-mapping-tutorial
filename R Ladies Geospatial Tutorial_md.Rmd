---
title: An R Markdown document converted from "C:/Users/argun/Documents/GitHub/r-ladies-mapping-tutorial/R
  Ladies Geospatial Tutorial_md.ipynb"
output: html_document
---

```{r}
suppressMessages(library(rgdal)) ###used for projections and importing data
suppressMessages(library(ggplot2)) ##creating the map
suppressMessages(library(sf)) ## data is stored as sf dataframes
suppressMessages(library(data.table)) 
suppressMessages(library(dplyr)) ##data transformations
suppressMessages(library(RColorBrewer)) #color palettes
suppressMessages(library(stringr)) ##string manipulation when filtering dataframes
suppressMessages(library(extrafont)) ##fonts for map
suppressMessages(loadfonts(device = "win")) ### fonts for map
suppressMessages(library(ggthemes)) ##map design
library(ggsn) ##scalebar
library(magrittr) ##pipes
library(classInt) ##intervals for legend
library(scales) ## commas in legend
library(grDevices) ### added for export to PDF
library(tmap) ###interactive mapping
```

### Importing Shapefile Data, Projecting, and Converting to R SF format

There are two ways you can download data:

1. Manually download

2. Get URL for download and do all processing in R


### Since the file names are so long and complicated, the process will be as follows:

- download the file

- unzip

- create a subfolder to process and rename the files in

- rename the file

- create a final folder where the processed and renamed files will go

### Boroughs

```{r}
## set wd
####ADD CODE HERE FOR FILEPATH
###PASTE FOLDER NAMES

###the open data shapefiles don't have clear names, so we need to do some pre-processing

##create a temporary folder for the download (only do this once, not for every file)
temp_folder= "C:/Users/argun/Documents/Shapefiles/NYC/Testing/temp"
dir.create(temp_folder)

###download the data (these are the borough boundaries)
download.file("https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/boroughs.zip",
              mode = "wb")

##unzip the folder
unzip("C:/Users/argun/Documents/Shapefiles/NYC/Testing/boroughs.zip",
      exdir=temp_folder)


```
```{r}
###we know all the downloads have the file pattern of "geo" with several characters after
old_files <- list.files("C:/Users/argun/Documents/Shapefiles/NYC/Testing/temp", full.names = TRUE)
old_files

```


```{r}
# rename the download
new_names = gsub("^.*?\\.","boroughs.",old_files)

###create new folder (only do this once, not for every file)
new_dir <- "C:/Users/argun/Documents/Shapefiles/NYC/Testing/shps"
dir.create(new_dir)

##move unzipped and clean files to a new folder
file.copy(from=old_files, to=paste0(new_dir, "/", new_names))
file.remove(old_files)


###let's check out the new files
list.files(new_dir)
```

### Bodies of Water

```{r}
###bodies of water

download.file("https://data.cityofnewyork.us/api/geospatial/5v6p-rtfu?method=export&format=Shapefile", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/bodies_of_water.zip",
              mode = "wb")

unzip("C:/Users/argun/Documents/Shapefiles/NYC/Testing/bodies_of_water.zip",
      exdir=temp_folder)
old_files <- list.files(temp_folder, pattern = "geo*", full.names = TRUE)

new_names = gsub("^.*?\\.","bodies_of_water.",old_files)
file.copy(from=old_files, to=paste0(new_dir, "/", new_names))
file.remove(old_files)
```

### Parks Properties

```{r}
###parks properties, NOT NTAS
download.file("https://data.cityofnewyork.us/api/geospatial/g84h-jbjm?method=export&format=Shapefile", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/parks_properties.zip",
              mode = "wb")

unzip("C:/Users/argun/Documents/Shapefiles/NYC/Testing/parks_properties.zip",
      exdir=temp_folder)
old_files <- list.files(temp_folder, pattern = "geo*", full.names = TRUE)
new_names = gsub("^.*?\\.","parks_properties.",old_files)
file.copy(from=old_files, to=paste0(new_dir, "/", new_names))
file.remove(old_files)
```

### NYC Neighborhoods (NTAs)

```{r}
####NTAS, eventually derive parks from this
download.file("https://data.cityofnewyork.us/api/geospatial/d3qk-pfyz?method=export&format=Shapefile", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/ntas.zip",
              mode = "wb")

unzip("C:/Users/argun/Documents/Shapefiles/NYC/Testing/ntas.zip",
      exdir=temp_folder)
old_files <- list.files(temp_folder, pattern = "geo*", full.names = TRUE)
new_names = gsub("^.*?\\.","ntas.",old_files)
file.copy(from=old_files, to=paste0(new_dir, "/", new_names))
file.remove(old_files)
```

### Read in All Those Shapefiles


```{r}
###read in and project shapefiles to 2263

shps <- "C:/Users/argun/Documents/Shapefiles/NYC/Testing/shps"
boroughs <- st_transform(st_read(shps, "boroughs" ), 2263)
ntas <- st_transform(st_read(shps, "ntas"), 2263)
parks = ntas %>% filter(str_detect(ntaname, paste0("park", collapse = "|")))
parks_properties <- st_transform(st_read(shps, "parks_properties"), 2263)
gc()
```



### Working With CSVs

#### csvs require a different type of processing; shapefiles are inherent spatial, excel files need to be converted
#### since csvs are not downloaded in a zip file, there is less processing involved

### Open Restaurant Applications - Download

```{r}
###download the csv

download.file("https://data.cityofnewyork.us/api/views/pitm-atqc/rows.csv?accessType=DOWNLOAD&bom=true&format=true", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/open_restaurants.csv",
              mode = "wb")

```

### Import and Process the Data

```{r}
### import the csv
rest <- fread("C:/Users/argun/Documents/Shapefiles/NYC/Testing/open_restaurants.csv", header = TRUE)
colnames(rest)
```

```{r}
###remove rows without lat/long data
rest <-  rest[!is.na(rest$Latitude) | !is.na(rest$Longitude), ]

###make the imported table in to a sf dataframe
rest_sf <- st_as_sf(rest , coords = c("Longitude", "Latitude"))

### remove old file
rm(rest)
```



### We've made it spatial, but let's check the coordinate system and see what it looks like plotted

```{r}
##check coordinate system
st_crs(rest_sf)
```

```{r}
### now let's see what it looks like
###uncomment boroughs to show that you cannot have two datasets with conflicting projections
ggplot()+
  geom_sf(data = rest_sf, color="black", size=.8)#+
  #geom_sf(data = boroughs, color="lightgrey")
```

### Why does the above data look distorted? Because we haven't assigned a coordinate system. Let's do that now

```{r}
st_crs(rest_sf)$IsGeographic ##to check is the CRS is geographic or not
st_crs(rest_sf)$units_gdal ##to find out the CRS units
st_crs(rest_sf)$srid ##extracts its ‘SRID’ identifier (when available)
st_crs(rest_sf)$proj4string ##extracts the proj4string representation
```


### Let's See What Happens When We Set the Coordinate Reference System

```{r}
### set coordinate reference system
###World Geodetic System, 99% of GPS systems use this, I have yet to use data that doesn't
sf_proj <- st_set_crs(rest_sf, value = 4326)
ggplot()+
  geom_sf(data = sf_proj, color="black", size=.8)

```

### No Longer Distorted, But We Can't Measure Distances Without a Projection

```{r}
st_crs(sf_proj)$units_gdal ##to find out the CRS units
```

### In Order to Properly Measure Distances, We Need to Project the Data to the Correct Geographic Area

```{r}
### set projection
sf_proj <- st_transform(sf_proj, "EPSG: 2263")

##check units
st_crs(sf_proj)$units_gdal ##to find out the CRS units
st_crs(sf_proj)$srid ##extracts its ‘SRID’ identifier (when available)
st_crs(sf_proj)$proj4string ##extracts the proj4string representation
rm(rest_sf)
```

## Now That The Geospatial Processing is Done, We Can Clean the Data as Needed

### Check for duplicates

```{r}
sf_proj %>% 
  group_by(objectid) %>% 
  filter(n()>1)
```

### Remove Duplicates

#### Here we create two files; a non-spatial data table and a spatial sf dataframe; this is to show two options
#### for joining the data later on

```{r}
rests_dt <- as.data.table(unique(sf_proj, by = "objectid")) ##data table version for dplyr aggregation
rests_sf <- unique(sf_proj, by = "objectid") ## sf version for spatial join

rm(sf_proj)

```

### Working With Files from a Geodatabase

### We're Going to Work with LION, which is a representation of NYC's Street Network and Other Boundaries

### Reading in Files from a Geodatabase

A geodatabase contains multiple files, so you need to know which file you're reading in. In this case, we only want NYC's road network

```{r}

##read lion from gdb
download.file("https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nyclion_21d.zip", 
              destfile = "C:/Users/argun/Documents/Shapefiles/NYC/Testing/nyclion_21d.zip",
              mode = "wb")

unzip("C:/Users/argun/Documents/Shapefiles/NYC/Testing/nyclion_21d.zip",
      exdir=new_dir)


gdb <-  "C:/Users/argun/Documents/Shapefiles/NYC/Testing/shps/lion/lion.gdb" ## make path an object

### let's check out what's in the file geodatabase
ogrListLayers(gdb) ### create an object that is a list of layers
#fc <- sf::st_read("C:/Users/Ayanthi/Documents/Shapefiles/NYC/lion/lion.gdb", layer = "lion") ## sometimes this works sometimes it doesn't??
```

### Based on the metadata, we know LION is the road network

```{r}
lion <- st_read(dsn=gdb,layer="lion") ## read in data

lion_condensed <- lion %>%
  ### remove railroads, water edge, census block boundary, paper street
  ### district boundary, alley, and ferry route
  filter(!FeatureTyp %in% c("1", "2", "3", "5", "7", "8", "A", "F" )) %>%
  filter(!RB_Layer %in% c("R", "S", "F")) ## remove roadbed, suppressed, and fake segments
lion_condensed <- st_transform(lion_condensed, 2263)
```
### Checking LION Geometry before st_cast command above
```{r}
st_geometry_type(lion, by_geometry = FALSE)
```
### LION Geometry after st_cast
```{r}
st_geometry_type(lion_cast, by_geometry = FALSE)
```
```{r}
rm(lion)
rm(lion_cast)
```

###Let's read in the rest of the data

```{r}

```

### Create a simple map using ggplot2 syntax

How do you think this design can be made better?

- remove streets (too many for something so zoomed out)
- add context via parks (explains the gaps)

```{r}
ggplot()+
  geom_sf(data=ntas, aes(fill="Neighborhood Boundaries"), color="black", lwd=.002)+
  #geom_sf(data=parks, fill="#a1c690", color="darkgrey")+
  geom_sf(data=parks_properties, aes(fill="Parks"), lwd=.002)+
  #geom_sf(data = lion_condensed, color="#f2f2f2", aes(color="Street"), size=.001)+
  geom_sf(data=rests_sf, aes(colour="Open Restaurant Permits"), color="red", size=.2, show.legend = "point", shape=16)+
  theme(panel.background = element_rect(fill = "white"),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_line(color = "white", size = 0.8),
        plot.title= element_text(colour="black", size = 24, face = "bold", hjust=0.5,),
        legend.position = "left",
        legend.spacing.y = unit(0, 'cm'))+
  ###legend for polygons
  scale_fill_manual(values = c("Parks" = "#a1c690", "Neighborhood Boundaries"="white"), name="Legend",
                    guide = guide_legend(override.aes = list(linetype = c("solid", "solid"), 
                                                               shape = c(NA, NA)))) +
  ###legend for points
  scale_color_manual(values = c("Open Restaurant Permits" = "red"), name = NULL,
                     guide=guide_legend(override.aes = list(shape = c(16),
                                                            size=2))) +
  
  labs(title = "Open Restaurant Permits \n New York City",
       caption = "Source: NYC Open Data, Retrieved on 2/22/2022")
```

The above map tells shows us the distribution of LPIs across NYC. But what does this mean to the average person? After a certain critical mass of points, you can't visually tell where there are more or less points, or how relevant those points are to the analysis you're trying to do

### We can make what's called a choropleth map, which aggregates points to a geometry and shows meaningful patterns


### Restaurants Grouped by NTA

There are two ways we can do this:

1. since the restaurants file has a NTA column we can use dplyr to get the counts by NTA

2. use a spatial join to count the number of points in polygons


### Note: You CANNOT use dplyr for grouping with spatial objects; even if you group by one column, it automatically also groups by the geometry of each point as well

### DPLYR Method: using the (non-spatial) datatable we created earlier

```{r}
rest_nta <- rests_dt %>% group_by(NTA) %>% summarise(count_restaurants = n())
nta_rest = merge(x=ntas,y=rest_nta,by.x="ntaname", by.y="NTA",all.x=TRUE)
head(nta_rest)
rm(rest_nta)
```

### Spatial Join Method

Add a column to the ntas data with the count of restaurants.

```{r}
ntas$count_restaurants <- lengths(st_intersects(ntas, rests_sf))
```


### Normalize the Data and Remove Neighborhoods That Are Outliers

```{r}
###normalize counts
nta_remove = c("cemetery", "Airport") ###list of outliers
nta_rest <- nta_rest %>%
filter(!grepl(paste(nta_remove, collapse="|"), ntaname)) %>% ##remove outliers
mutate(rest_sqmi= count_restaurants/(shape_area/27878400)) ###get restaurants per square mile


### keep the removed neighborhoods as a separate df
nta_removed <- ntas %>%
filter(grepl(paste(nta_remove, collapse="|"), ntaname)) 
```

### Create a choropleth map from this

We don't want every value in restaurants/sq mile to be a separate value, it doesn't tell us much

```{r}
ggplot(nta_rest) + 
    geom_sf(aes(fill=rest_sqmi))+
    scale_fill_viridis_c(option = "D", na.value = "darkgrey") +
    geom_sf(data=nta_removed, fill="darkgrey")+
    geom_sf(data=parks, fill="#a1c690", color="darkgrey")+
    theme(panel.background = element_rect(fill = "white"),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_line(color = "white", size = 0.8),
        plot.title= element_text(colour="black", size = 24, face = "bold", hjust=0.5,))+
  labs(title = "Open Restaurant Permits Per Square Mile \n New York City")
```

### In Order to Better Represent this Data, Let's Add Quantile Breaks

```{r}
# get quantile breaks.
breaks_qt <- classIntervals(c(min(nta_rest$rest_sqmi), nta_rest$rest_sqmi), n = 5, style = "quantile")

### use the "cut" function to add a breaks column in your sf object
nta_rest <- mutate(nta_rest, brks = cut(rest_sqmi, breaks_qt$brks, include.lowest = TRUE, ,dig.lab=4)) 

breaks_qt
```

```{r}
nta_rest[which(is.na(nta_rest$brks)),]
```

```{r}

ggplot(nta_rest) + 
    geom_sf(aes(fill=brks)) +
    scale_fill_brewer(palette = "OrRd", na.value="darkgrey") +
    geom_sf(data=parks, fill="#a1c690", color="darkgrey")+
    geom_sf(data=nta_removed, fill="darkgrey")
```

```{r}
library(tmap)
tm_shape(nta_rest) +
  tm_polygons("rest_sqmi", 
              style="quantile", 
              legend.format = list(text.separator="-", fun = function(x) formatC(x, digits = 2, big.mark = ",", format = "f")) ,
              title="New York City\nOpen Restaurant Permits \nper Square Mile")+
tm_shape(nta_removed)+
  tm_polygons(col = "darkgrey") +
tm_shape(parks)+
  tm_polygons(col="#a1c690", border.col = "darkgrey")
```

### But what if we want a closeup?

```{r}
####data for zoomed in map

ntas_cropped <- st_crop(ntas, xmin = 994462.6, xmax = 1005982.5,
                        ymin = 181378.1, ymax = 188278.7)
rest_cropped <- st_intersection(sf_proj, ntas_cropped)
streets_cropped <- st_intersection(lion_condensed, ntas_cropped)
parks_properties_cropped <- st_intersection(parks_properties, ntas_cropped) %>%
  subset(!landuse %in% c("Mall", "Parkway"))
```

```{r}
ggplot()+
  geom_sf(data=ntas_cropped, fill="white")+
  #geom_sf(data=parks_cropped, fill="#a1c690", color="lightgrey")+
  geom_sf(data = streets_cropped, color="#f2f2f2", size=.005)+
  geom_sf(data=rest_cropped, color="red", size=.2)+
  theme(panel.background = element_rect(fill = "white"),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_line(color = "white", size = 0.8))
```

### Watch What Happens When you Try and Add Street Labels

```{r}
ggplot()+
  geom_sf(data=ntas_cropped, fill="white")+
  #geom_sf(data=parks_cropped, fill="#a1c690", color="lightgrey")+
  geom_sf(data = streets_cropped, color="#f2f2f2", size=.005)+
  geom_sf_text(data=streets_cropped, 
               aes(label=Street, family="serif"), colour = "black", size=2)+
  geom_sf(data=rest_cropped, color="red", size=.2)+
  theme(panel.background = element_rect(fill = "white"),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.grid = element_line(color = "white", size = 0.8))
```

### Processing for Labeling Roads

Each section of the road between two intersections is a separate line in the data. Therefore, there are several lines
that represent each street

```{r}
###union roads for the purpose of labeling

roads.out <- streets_cropped %>% 
  group_by(Street) %>% 
  summarize(geometry = st_union(SHAPE))
```

```{r}
#g <- 
ggplot() + 
  geom_sf(data=streets_cropped, aes(color="streets"), show.legend = "line", size=.005)+
  #geom_sf(data = ntas_cropped, fill=NA, color="black")+
  geom_sf(data=parks_properties_cropped, fill="#a1c690", color=NA)+
  geom_sf_text(data=roads.out[roads.out$Street=="ATLANTIC AVENUE",], 
               aes(label=Street, family="serif"), colour = "black", size=2, angle=-5
               ,nudge_x=1600, nudge_y=-35)+
  geom_sf_text(data=roads.out[roads.out$Street=="FRANKLIN AVENUE",], 
               aes(label=Street, family="serif"), colour = "black", size=2, 
               angle=80 ,
               nudge_x = -70, nudge_y = 350)+
  geom_sf_text(data=roads.out[roads.out$Street=="EASTERN PARKWAY",], 
               aes(label=Street, family="serif"), colour = "black", size=2, angle=-5,
              nudge_x = 5000, nudge_y=-750)+
  geom_sf(data=rest_cropped, aes(color="Restaurants"), show.legend = "point", size=1.2)+
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(),
        panel.background = element_blank(),
        plot.background = element_rect(fill = "white"),#, color = "grey20"),size = 2),
        plot.title= element_text(colour="black", size = 24, face = "bold", hjust=0.5,),
        legend.position = "right") +
  theme(legend.title = element_blank(),
        legend.spacing.y = unit(0, "mm"), 
        panel.border = element_rect(colour = "black", fill=NA),
        aspect.ratio = 1, axis.text = element_text(colour = 1, size = 12),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black"))+
  labs(title = "Open Restaurant Permits \n Crown Heights North")+
  xlab("") + 
  ylab("")+
  scale_colour_manual(values = c("Restaurants" = "red", "streets"="darkgrey"),
                      guide = guide_legend(override.aes = list(linetype =  c("blank", "solid"),
                                                              shape=c(16, NA))))+
  scalebar(ntas_cropped, dist = .5, dist_unit = "mi", location="bottomleft",
             transform = FALSE, model = "WGS84")
  
```

```{r}
ggsave("test_map_res.pdf",
       plot=g,
       width=8.5, height=11, units="in",
       dpi=1200)
#unlink("test_map_rest.pdf")
```

```{r}
dev.off()
```
```{r}
tmap_mode("view")
#tm_shape(ntas)+
#  tm_polygons(col="white", alpha = .5)+
tmap_leaflet(tm_shape(rest_sf)+
  tm_symbols(shape=21, col="red", size = .0005,
  popup.vars=c("Restaurant Name"="Restaurant Name", "Seating Interest (Sidewalk/Roadway/Both)"="Seating Interest (Sidewalk/Roadway/Both)")))
```

### Cleanup

```{r}
removals <- list.files("C:/Users/argun/Documents/Shapefiles/NYC/Testing", pattern = "*.zip*", full.names = TRUE)
file.remove(removals)
```

